{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this task was to use crime data from Chicago's Data Portal (Crimes - 2001 to present) to train a supervised learning model that could predict the occurrence of a crime in Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6856243\n",
      "THEFT                                288892\n",
      "BATTERY                              249885\n",
      "CRIMINAL DAMAGE                      156209\n",
      "NARCOTICS                            143466\n",
      "OTHER OFFENSE                         85551\n",
      "ASSAULT                               85414\n",
      "BURGLARY                              78371\n",
      "MOTOR VEHICLE THEFT                   63526\n",
      "DECEPTIVE PRACTICE                    54219\n",
      "ROBBERY                               51918\n",
      "CRIMINAL TRESPASS                     39177\n",
      "WEAPONS VIOLATION                     14575\n",
      "PROSTITUTION                          13734\n",
      "PUBLIC PEACE VIOLATION                 9795\n",
      "OFFENSE INVOLVING CHILDREN             9351\n",
      "CRIM SEXUAL ASSAULT                    5734\n",
      "SEX OFFENSE                            5262\n",
      "INTERFERENCE WITH PUBLIC OFFICER       3218\n",
      "GAMBLING                               2930\n",
      "LIQUOR LAW VIOLATION                   2745\n",
      "ARSON                                  2189\n",
      "HOMICIDE                               1935\n",
      "KIDNAPPING                             1351\n",
      "INTIMIDATION                            780\n",
      "STALKING                                717\n",
      "OBSCENITY                               122\n",
      "CONCEALED CARRY LICENSE VIOLATION        65\n",
      "NON-CRIMINAL                             43\n",
      "OTHER NARCOTIC VIOLATION                 28\n",
      "PUBLIC INDECENCY                         22\n",
      "HUMAN TRAFFICKING                        11\n",
      "RITUALISM                                 4\n",
      "NON-CRIMINAL (SUBJECT SPECIFIED)          4\n",
      "NON - CRIMINAL                            4\n",
      "Name: Primary_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# The data to load\n",
    "f = \"crime_data/Crimes_-_2001_to_present.csv\"\n",
    "\n",
    "# Code to load random sample of .csv\n",
    "num_lines = sum(1 for l in open(f))\n",
    "print(num_lines)\n",
    "# Sample size - in this case ~20% - Anymore than this I run into memory issues\n",
    "size = int(num_lines / 5)\n",
    "skip_idx = random.sample(range(1, num_lines), num_lines - size)\n",
    "\n",
    "# Read the data\n",
    "## According to Chicago Data Portal, 'Community Areas' is current. I am going to remove 'Community Area'.\n",
    "drops = ['Case Number', 'Block',  'Description',  'Location Description', 'Updated On', 'Community Area']\n",
    "data = pd.read_csv(f, skiprows=skip_idx).drop(drops, axis=1)\n",
    "data.columns = ['ID','Date','IUCR',\n",
    "                     'Primary_Type','Arrest','Domestic', 'Beat', 'District',\n",
    "                     'Ward', 'FBI_Code', 'X_Coordinate', 'Y_Coordinate',\n",
    "                     'Year', 'Latitude', 'Longitude', 'Location', 'Historical_Wards',\n",
    "                     'Zip Codes', 'Community_Areas', 'Census_Tracts', 'Wards', 'Boundaries_ZIP',\n",
    "                     'Police_Dist', 'Police_Beats']\n",
    "print(data['Primary_Type'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is part of the **eda.py** program. This program limits the crime data (*Crimes_-_2001_to_present.csv*) to just a random 20% sample of the original rows. Working with any larger of a sample resulted in memory issues. \n",
    "The next step was to get a sense of the types of crimes contained in these data. By looking at the \"Primary Type\" column I could see a standardized description of the type of crime each observation in the data represents. The data contain a wide array of offenses. To build a model that could predict every type of crime would require many different features. Thus, I felt it was appropriate to limit this exercise to just looking at one type of crime. I chose Burglary since it is a common offense in Chicago, and I felt I could develop an appropriate set of model features for predicting this type of crime.\n",
    "These sample data were then limited to just burglaries and output to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to Predict Burglary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program **burglaries_predictor_variables.py** takes the dataset described in the last step and adds some columns that will be used as features for our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Time Realted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each observation, how much time has elapsed since the last burglary in the same community area (in days)\n",
    "* Dummy variable to indicate working hours of the day (8am - 7pm)\n",
    "* Dummy variable to indicate colder (winter) months (October to March)\n",
    "* Dummy variable to indicate work days of the week (M - F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Geographical Features (Community Area Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average amount of time that elapses between bruglaries within a community area (in days)\n",
    "* Dummy variables to indicate each community area\n",
    "* Total number of burglaries in that community area\n",
    "* Number of affordable housing units within that community area\n",
    "    * The data on affordable housing units by community area were obtained from here: https://data.cityofchicago.org/Community-Economic-Development/Affordable-Housing-Units-by-Community-Area/yvj4-y3fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Law Enforcement Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of police beats (from crimes dataset)\n",
    "* Haversine  distance to nearest police station (in kilometers)\n",
    "    * The latitude and longitude for police station locations in Chicago were obtained from: https://data.cityofchicago.org/Public-Safety/Police-Stations/z8bn-74gv\n",
    "    * Haversine distances were calculated from each burglary to each police station. Then for each burglary, the minimum of the distances calculated were used as the distance to the nearest police station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to build a model that could predict the occurrence of a burglary within a certain future timeframe. More specifically I wanted to identify whether a burglary will happen in a community area within the next two weeks. The outcome variable is a binary outcome, taking on 1 if a burglary happened within 14 days of the last burglary in the community area, or 0 otherwise. Since the outcome is categorical, I would need to use a classifier algorithm. I chose to use a random forest model for several reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first reason I chose to use a random forest model was that it is easy to optimize the feature selection of the model. My approach, as shown below, would be to first run the random forest classifier with all the features I described in the previous section, score the algorithm, identify the most important features, then repeat the same classification algorithm using only the most important features, with gini-importance above a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second reason I chose to use a random forest is that it resolves the issues of using a \"greedy algorithm\", like with implementing a decision tree, as we are using many decision trees to prevent our results from being biased by one decision tree outcome. In addition to this we are not susceptible to overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model With All Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8850717012276901\n",
      "          Predict No  Predict Yes\n",
      "True No          370         1888\n",
      "True Yes         340        16788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n = 90\n",
    "#\n",
    "data = pd.read_csv(\"training_data_burglary.csv\")\n",
    "Training_Target = np.where((data['day_delta'] < 14), 1, 0)\n",
    "Training_Data = data.iloc[:,np.r_[32:115]]\n",
    "\n",
    "data_training, data_test, target_training, target_test = train_test_split(Training_Data, Training_Target, test_size = 0.25, random_state=1)\n",
    "random_forest_machine = RandomForestClassifier(n_estimators=n)\n",
    "random_forest_machine.fit(data_training, target_training)\n",
    "predictions = random_forest_machine.predict(data_test)\n",
    "print(accuracy_score(target_test, predictions))\n",
    "\n",
    "cm = confusion_matrix(target_test,predictions)\n",
    "confusion_matrix = pd.DataFrame(\n",
    "\tcm,\n",
    "\tcolumns = ['Predict No', 'Predict Yes'],\n",
    "\tindex = ['True No', 'True Yes']\n",
    ")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model scored ~0.88507. As we can see with the model with all features, our random forest algorithm correctly predicted the occurrence of a burglary 16,788 times and failed to predict a burglary only 340 times. However, when burglaries didn't happen it tended to make a type I error and predict a burglary would be present, when in fact a burglary did not occur within the 14 day timespan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'working_hours': 0.04789174542687778, 'winter': 0.046186494053002236, 'work_day': 0.04764561015895365, '10.0_area': 0.000298251158043319, '11.0_area': 0.002089878614287866, '12.0_area': 0.0035477153801159824, '13.0_area': 0.0007358934919541963, '14.0_area': 0.0007192470711610245, '15.0_area': 0.0001940991544416193, '16.0_area': 0.0002586259411464016, '17.0_area': 0.0007405095419473555, '18.0_area': 0.00407823727323944, '19.0_area': 0.00010158212693600333, '2.0_area': 0.00886888583788409, '20.0_area': 0.0002762760456842934, '21.0_area': 0.000751670997166664, '22.0_area': 0.00047848924929175604, '23.0_area': 0.0002953507295420248, '24.0_area': 0.0003191672594633944, '25.0_area': 0.0027972920320526733, '26.0_area': 0.0036152504435573393, '27.0_area': 0.0014157420329772032, '28.0_area': 0.0006771306899016116, '29.0_area': 0.0002795063129694009, '3.0_area': 0.004465321100533504, '30.0_area': 0.00033184716633579905, '31.0_area': 0.001247265813476658, '32.0_area': 0.0004347372358669323, '33.0_area': 0.001079995619374803, '34.0_area': 0.0036016526215003446, '35.0_area': 0.007839566724140002, '36.0_area': 0.0038788022731892735, '37.0_area': 0.00041049220556577495, '38.0_area': 0.0006719239276040573, '39.0_area': 0.003564470336390651, '4.0_area': 0.00035159514927497293, '40.0_area': 5.433662704428896e-05, '41.0_area': 0.0009509100378531102, '42.0_area': 0.00012974731212595382, '43.0_area': 0.002090225895247328, '44.0_area': 0.0012829952754575293, '45.0_area': 0.000341585088119722, '46.0_area': 0.0012902812773562462, '47.0_area': 0.006232812894231347, '48.0_area': 0.0006858380618765879, '49.0_area': 0.0009642442586094603, '5.0_area': 0.0013175010685364374, '50.0_area': 0.00013349982516217213, '51.0_area': 0.007069134030110839, '52.0_area': 0.0022490729835156495, '53.0_area': 0.0007910493828961455, '54.0_area': 0.0013016475862067478, '55.0_area': 0.00025324804617812786, '56.0_area': 0.002648998986690061, '57.0_area': 0.0005826969283428897, '58.0_area': 0.0012690484012540838, '59.0_area': 0.00012271746156044297, '6.0_area': 0.0009509681757306284, '60.0_area': 0.0010922801294872286, '61.0_area': 0.0006013475517643263, '62.0_area': 0.0025086765715502564, '63.0_area': 0.0017328460843793892, '64.0_area': 0.0006181034260128298, '65.0_area': 0.0007133594310907858, '66.0_area': 0.00010828832426169903, '67.0_area': 9.072180850887217e-05, '68.0_area': 0.00016930285226848498, '69.0_area': 0.0005363805073444684, '7.0_area': 0.00119714041316696, '70.0_area': 0.0002803938416964371, '71.0_area': 0.0010253088688390779, '72.0_area': 0.00047903034219682884, '73.0_area': 0.00540196694951957, '74.0_area': 0.000874625788972191, '75.0_area': 0.003262535920173115, '76.0_area': 0.0006312924264581484, '77.0_area': 0.0027903868329100918, '8.0_area': 0.0011660390864080168, '9.0_area': 0.00021311218916460114, 'CA_AVG_DELTA': 0.2620314073339472, 'CA_COUNT': 0.2622971059733229, 'LI_COUNT': 0.009124387907495136, 'police': 0.2062010786391055}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(Training_Data.columns, random_forest_machine.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows that a lot of our features aren't contributing much to the decision-making process for our algorithm. At first glance only a small number of features such as average time between burglaries (*CA_AVG_DELTA*) have a relatively high importance. The steps below will describe what was done to limit to only important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police          1\n",
      "CA_COUNT        1\n",
      "CA_AVG_DELTA    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Train Model to Select Most Important Features and Refine The Model\n",
    "# Fine Best Features\n",
    "select_features = SelectFromModel(random_forest_machine, threshold = 0.1)\n",
    "select_features.fit(data_training, target_training)\n",
    "x_refined_train = select_features.transform(data_training)\n",
    "x_refined_test = select_features.transform(data_test)\n",
    "\n",
    "# Print Top Features\n",
    "important_features = select_features.get_support()\n",
    "feature_name = Training_Data.columns[important_features]\n",
    "print(feature_name.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code does the following:\n",
    "1) SelectFromModel detects the features of our previous model that have a feature importance above 0.1\n",
    "2) The training and test data are then transformed to only take the features of the required importance\n",
    "3) Print a list of the top features\n",
    "    * Police Beat Count\n",
    "    * Community Burglary Count\n",
    "    * Community Average Time Between Burglaries\n",
    "\n",
    "Below we will re-run the random forest classifier with only these three features.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889817393995667\n",
      "          Predict No  Predict Yes\n",
      "True No          362         1896\n",
      "True Yes         240        16888\n"
     ]
    }
   ],
   "source": [
    "#Train and Test New Model\n",
    "refined_forest_machine = RandomForestClassifier(n_estimators=n)\n",
    "refined_forest_machine.fit(x_refined_train, target_training)\n",
    "refined_predictions = refined_forest_machine.predict(x_refined_test)\n",
    "print(accuracy_score(target_test, refined_predictions))\n",
    "\n",
    "# for some reason i could not run this part of the code until i imported the confusion matrix library again\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmr = confusion_matrix(target_test,refined_predictions)\n",
    "confusion_matrix = pd.DataFrame(\n",
    "\tcmr,\n",
    "\tcolumns = ['Predict No', 'Predict Yes'],\n",
    "\tindex = ['True No', 'True Yes'])\n",
    "\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for the refined model improved marginally to ~0.88981. The confusion matrix however indicates the model became better at predicting burglaries when they actually happened, but would still overpredict a burglary happening when it did not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
